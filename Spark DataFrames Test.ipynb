{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test PySpark & DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be asked some basic questions about some stock market data from the years 2012-2017. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the stock.csv file to Answer and complete the  tasks below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Operations\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Stock CSV File, have Spark infer the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('stock.csv',inferSchema=True,header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List the 5th and 6th column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns[4:6])\n",
    "\n",
    "# or \n",
    "# print(df.select(['Close', 'Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the Schema and change the inferred Volume type from integer to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (DoubleType, TimestampType)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column already provides cast method with DataType instance\n",
    "\n",
    "df = df.withColumn(\"Volume\", df[\"Volume\"].cast(\"double\"))\n",
    "df = df.withColumn(\"Date\", df[\"Date\"].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the expected output\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the date and the open price of the first 5 rows as follows.\n",
    "on 2012-01-03 00:00:00 , the open price was 59.97 $\n",
    "\n",
    "\n",
    "on 2012-01-04 00:00:00 , the open price was 60.21 $\n",
    "\n",
    "\n",
    "on 2012-01-05 00:00:00 , the open price was 59.35 $\n",
    "\n",
    "\n",
    "on 2012-01-06 00:00:00 , the open price was 59.42 $\n",
    "\n",
    "\n",
    "on 2012-01-09 00:00:00 , the open price was 59.03 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.head(5):\n",
    "  print(f'on {row[\"Date\"]} , the open price was {round(row[\"Open\"], 2)} $')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use describe() to generate a summary DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the expected output\n",
    "\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are too many decimal places for mean and stddev in the describe() dataframe. Format the numbers to just show up to two decimal places. Pay careful attention to the datatypes that .describe() returns. [Check this link for a hint](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.cast.html?highlight=cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DecimalType\n",
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.select(result['summary'],format_number(result['Open'].cast('float'),0).alias('Open'),\n",
    "             format_number(result['High'].cast('float'),2).alias('High'),\n",
    "             format_number(result['Low'].cast('float'),2).alias('Low'),\n",
    "             format_number(result['Close'].cast('float'),2).alias('Close'),\n",
    "             format_number(result['Volume'].cast('float'),2).alias('Volume')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new dataframe with 2 columns : \n",
    "##### HV Ratio that is the ratio of the High Price versus volume of stock traded for a day,  \n",
    "##### and LV Ratio that is the ratio of the Low Price versus volume of stock traded for a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the expected output\n",
    "\n",
    "df_2 = df.withColumn('HV Ratio', df['High']/df['Volume'])\n",
    "df_3 = df.withColumn('LV Ratio', df['Low']/df['Volume'])\n",
    "\n",
    "df_2.select('HV Ratio').show(20)\n",
    "df_3.select('LV Ratio').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What day had the Highest HV Ratio? and what day we had the lowest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the expected output\n",
    "df_2.orderBy(df_2[\"HV Ratio\"].desc()).head(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.orderBy(df_2[\"HV Ratio\"]).head(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the mean of the LV Ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the expected output\n",
    "\n",
    "from pyspark.sql.functions import mean\n",
    "df_3.select(mean('LV Ratio')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many days volume was greater than 9000000 and the Close was lower than 70$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = filter(lambda x: x[\"Volume\"] > 9000000 and x[\"Close\"] < 70, df.collect())\n",
    "len(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What percentage of the time was the difference between High and Low greater than 1 dollars ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_one_dollar = filter(lambda x: abs(x[\"High\"] - x[\"Low\"]) > 1, df.collect())\n",
    "filtered_df_all_differences = filter(lambda x: x[\"High\"] != x[\"Low\"], df.collect())\n",
    "more_than_one_dollar_difference_count = len(list(filtered_df_one_dollar))\n",
    "all_difference_count = len(list(filtered_df_all_differences))\n",
    "round(more_than_one_dollar_difference_count / (all_difference_count / 100),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the Pearson correlation between Close and Volume?\n",
    "#### [Hint](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.corr.html?highlight=corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the expected output\n",
    "\n",
    "from pyspark.sql.functions import corr\n",
    "df.select(corr('Close', 'Volume')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the max and min HV Ratio per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (year, max, min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeardf = df_2.withColumn(\"Year\", year(df_2['Date']))\n",
    "yeardf2 = df_2.withColumn(\"Year\", year(df_2['Date']))\n",
    "\n",
    "max_df = yeardf.groupBy('Year').max()\n",
    "max_df2 = yeardf2.groupBy('Year').min()\n",
    "\n",
    "# here is the expected output\n",
    "\n",
    "max_df.select('Year', 'max(HV Ratio)').show()\n",
    "max_df2.select('Year', 'min(HV Ratio)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the highest Close for each Calendar Month?\n",
    "#### In other words, across all the years, what is the highest Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthdf = df.withColumn('Month', month('Date'))\n",
    "month_high = monthdf.select(['Month', 'Close']).groupBy('Month').max()\n",
    "\n",
    "# here is the expected output\n",
    "\n",
    "month_high.select('Month', 'max(Close)').orderBy('Month').show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
